{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05a4b5ee-1746-4dc7-be1d-831f7448ef4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "random.seed(5) #for reproducability\n",
    "chunk_size = 256 #configure as needed\n",
    "columns_for_train = ['1.0 std','2.5 std', '10 std'] #choose what to train on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "944f3f2e-bd2f-4c83-9469-89ea641c0191",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Linear_Scale(dataframe, column): #normal absolute max normalization\n",
    "    dataframe[column] = (dataframe[column] - dataframe[column].min()) / (dataframe[column].max() - dataframe[column].min())\n",
    "    \n",
    "def Z_norm(dataframe, column): #see z-score normalization, better for outlier detection\n",
    "    dataframe[column] = (dataframe[column] - dataframe[column].mean()) / dataframe[column].std()\n",
    "\n",
    "def dfsplitter(dflist): #receives iterable list of dataframe 'chunks', and returns three separate lists containing 70%, 20% and final 10% of the chunks\n",
    "    percent70 = int(len(dflist) * 0.7) #number of chunks = length of list, convert to int to round down any decimals\n",
    "    percent20 = int(len(dflist) * 0.2)\n",
    "    \n",
    "    first70 = dflist[0 : percent70] #recieve items in big list from 0 to 70%\n",
    "    next20 = dflist[percent70 : percent70 + percent20] #from 70 to 70+20 = 90%\n",
    "    last10 = dflist[percent70 + percent20 : ] #whatever is left. will not always be exactly 10% due to earlier rounding\n",
    "    return(first70, next20, last10) \n",
    "\n",
    "def combine_and_shuffle(list1, list2): #take two lists of dataframes, combine them, shuffle the new list, and concatenate into a single dataframe\n",
    "    dflist = list1 + list2\n",
    "    random.shuffle(dflist)\n",
    "    return pd.concat(dflist, ignore_index=True)\n",
    "\n",
    "def df_to_Xy(dataframe): #converts a given dataframe to X and y numpy arrays, used for training, testing and validation\n",
    "    X, y = dataframe.loc[:, columns_for_train], dataframe.loc[:, 'Fire'] #X is for train, y is boolean fire values\n",
    "    \n",
    "    X, y = X.to_numpy(), y.to_numpy() #convert each to numpy arrays\n",
    "    \n",
    "    y = y.flatten() #reshaping each\n",
    "    X = X.reshape((X.shape[0], X.shape[1], 1)) #maintain rows, number of columns, and add third dimenion (1 item each)\n",
    "    return(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a5da3184-c7e2-46a1-a878-943d28502f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(5) #reproducability\n",
    "\n",
    "df = pd.read_csv('processed.csv', chunksize=chunk_size, iterator=True) #df is now a TextFileReader dtype\n",
    "allchunks = [pd.DataFrame(chunk) for chunk in df]\n",
    "random.shuffle(allchunks) #a list of chunk-sized dataframes, randomized order\n",
    "\n",
    "empty = pd.read_csv('empty.csv', chunksize=chunk_size, iterator=True) #TextFileReader again\n",
    "allempties = []\n",
    "\n",
    "for chunk in empty:\n",
    "    chunk.loc[chunk.sample(frac=0.004, random_state=2).index, columns_for_train] = 5996 #take a sample of each chunk and convert all values in the row\n",
    "    for column in columns_for_train: #for each column in each chunk:\n",
    "        Z_norm(chunk, column)\n",
    "        Linear_Scale(chunk, column)\n",
    "    allempties.append(pd.DataFrame(chunk)) #add the chunk as a dataframe to a list, resulting in another list of dataframes\n",
    "\n",
    "\n",
    "first70, next20, last10 = dfsplitter(allchunks) #split first df list into training, validation and testing sizes\n",
    "empty70, empty20, empty10 = dfsplitter(allempties) #same thing for list of error values ('empty' as in no fire spikes)\n",
    "\n",
    "final70 = combine_and_shuffle(first70, empty70) #take two df lists, combine, shuffle, concatenate into a DataFrame\n",
    "final20 = combine_and_shuffle(next20, empty20)\n",
    "final10 = combine_and_shuffle(last10, empty10)\n",
    "#plt.plot(final70.index[:], final70['1.0 std'][:], final70['Fire'][:]) #for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8bc5bd52-7ab3-45b3-b817-2a1958cbcd54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((26112, 3, 1), (26112,), (7174, 3, 1), (7174,), (4001, 3, 1), (4001,))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train = df_to_Xy(final70) #take train-sized DataFrame, pull data and boolean values, and convert to X and y\n",
    "X_val, y_val = df_to_Xy(final20)\n",
    "X_test, y_test = df_to_Xy(final10)\n",
    "\n",
    "X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8aadb906-4eb5-470d-b3dd-934d4c7cfe82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ gru (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)         │       <span style=\"color: #00af00; text-decoration-color: #00af00\">791,040</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">591,360</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">514</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ gru (\u001b[38;5;33mGRU\u001b[0m)                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m512\u001b[0m)         │       \u001b[38;5;34m791,040\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru_1 (\u001b[38;5;33mGRU\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m591,360\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │           \u001b[38;5;34m514\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,382,914</span> (5.28 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,382,914\u001b[0m (5.28 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,382,914</span> (5.28 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,382,914\u001b[0m (5.28 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Epoch 1/10\n",
      "\u001b[1m816/816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 18ms/step - accuracy: 0.9756 - loss: 0.1082 - val_accuracy: 0.9869 - val_loss: 0.0407\n",
      "Epoch 2/10\n",
      "\u001b[1m816/816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.9798 - loss: 0.0465 - val_accuracy: 0.9869 - val_loss: 0.0308\n",
      "Epoch 3/10\n",
      "\u001b[1m816/816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 20ms/step - accuracy: 0.9848 - loss: 0.0377 - val_accuracy: 0.9873 - val_loss: 0.0306\n",
      "Epoch 4/10\n",
      "\u001b[1m816/816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 17ms/step - accuracy: 0.9858 - loss: 0.0335 - val_accuracy: 0.9884 - val_loss: 0.0249\n",
      "Epoch 5/10\n",
      "\u001b[1m816/816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.9866 - loss: 0.0316 - val_accuracy: 0.9912 - val_loss: 0.0205\n",
      "Epoch 6/10\n",
      "\u001b[1m816/816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 20ms/step - accuracy: 0.9880 - loss: 0.0303 - val_accuracy: 0.9907 - val_loss: 0.0232\n",
      "Epoch 7/10\n",
      "\u001b[1m816/816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.9885 - loss: 0.0282 - val_accuracy: 0.9932 - val_loss: 0.0247\n",
      "Epoch 8/10\n",
      "\u001b[1m816/816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 17ms/step - accuracy: 0.9878 - loss: 0.0291 - val_accuracy: 0.9923 - val_loss: 0.0209\n",
      "Epoch 9/10\n",
      "\u001b[1m816/816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.9885 - loss: 0.0272 - val_accuracy: 0.9937 - val_loss: 0.0209\n",
      "Epoch 10/10\n",
      "\u001b[1m816/816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 20ms/step - accuracy: 0.9894 - loss: 0.0262 - val_accuracy: 0.9897 - val_loss: 0.0248\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9908 - loss: 0.0279\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.034934915602207184, 0.9880030155181885]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.Input(shape=(3, 1))) #no specific amount of timesteps, 3 pixels in each row/timestep\n",
    "model.add(layers.GRU(512, return_sequences=True, activation='relu')) #returns the output of each timestep so we can stack multiple RNN layers\n",
    "model.add(layers.GRU(256, activation='relu')) #no return sequences so that normal dense layers can proceed it\n",
    "model.add(layers.Dense(2))\n",
    "\n",
    "#could do LSTM, GRU, or RNN and can change by literally just swapping them out\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "#RNNs typically use tanh instead of relu\n",
    "\n",
    "model.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), #fromlogits is true since we don't have softmax activation in final dense layer\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001), #dont make learning rate too high or will result in overfitment of training data\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=32, validation_data=(X_val, y_val),epochs=10, verbose=1)\n",
    "\n",
    "model.evaluate(X_test, y_test, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b425bbd4-221e-469f-b099-35f8b8604b8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
